# -*- coding: utf-8 -*-
"""Copy of eDNA PRE_SIH.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CVYBlnW5PzBwLgEbTkwDguWCoZx_g9WM
"""
import matplotlib
matplotlib.use('Agg') 
import os
import numpy as np
import random
from collections import Counter
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import product

# Set random seeds for reproducibility
np.random.seed(42)
random.seed(42)

class eDNAPipeline_Data:
    # Changed: __init__
    def __init__(self, n_sequences=20000, sequence_length=200, k=4):
        self.n_sequences = n_sequences
        self.sequence_length = sequence_length
        self.k = k
        self.sequences = None
        self.kmer_vectors = None
        # Changed: Simpler path for Colab
        self.plots_dir = 'plots'
        os.makedirs(self.plots_dir, exist_ok=True)

    def generate_mock_edna_sequences(self):
        print("--- Step 1: Generating mock eDNA sequences... ---")
        sequences = []
        bases = ['A', 'T', 'G', 'C']
        templates = [''.join(np.random.choice(bases, size=self.sequence_length)) for _ in range(50)]

        for i in range(self.n_sequences):
            if i < self.n_sequences * 0.7:
                base_template = random.choice(templates)
                sequence = list(base_template)
                n_mutations = np.random.randint(int(0.15 * self.sequence_length), int(0.25 * self.sequence_length))
                mutation_positions = np.random.choice(self.sequence_length, size=n_mutations, replace=False)
                for pos in mutation_positions:
                    sequence[pos] = np.random.choice(bases)
                sequences.append(''.join(sequence))
            else:
                sequences.append(''.join(np.random.choice(bases, size=self.sequence_length)))

        self.sequences = sequences
        print(f"Generated {len(sequences)} mock eDNA sequences of length {self.sequence_length}")
        return sequences

    def sequences_to_kmers(self, sequences):
        print(f"\n--- Step 2: Converting sequences to {self.k}-mer vectors... ---")
        bases = ['A', 'T', 'G', 'C']
        all_kmers = [''.join(p) for p in product(bases, repeat=self.k)]
        kmer_to_index = {kmer: i for i, kmer in enumerate(all_kmers)}

        kmer_matrix = np.zeros((len(sequences), len(all_kmers)))

        for seq_idx, sequence in enumerate(sequences):
            kmer_counts = Counter([sequence[i:i + self.k] for i in range(len(sequence) - self.k + 1)])
            total_kmers = sum(kmer_counts.values())
            if total_kmers > 0:
                for kmer, count in kmer_counts.items():
                    if kmer in kmer_to_index:
                        kmer_matrix[seq_idx, kmer_to_index[kmer]] = count / total_kmers

        self.kmer_vectors = kmer_matrix
        print(f"Created k-mer matrix of shape {kmer_matrix.shape}")
        return kmer_matrix

# --- Execute the data generation process ---
# Changed: Removed the 'if __name__ == "__main__":' wrapper
print("Running Pair 1's data module...")
pipeline_data = eDNAPipeline_Data()
sequences = pipeline_data.generate_mock_edna_sequences()
kmer_vectors = pipeline_data.sequences_to_kmers(sequences)
print("\nâœ… Cell 1 execution complete. 'pipeline_data' object is ready.")

import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ===================================================================
# 1. VAE Model Definition ðŸ¤–
# ===================================================================
class VAE(keras.Model):
    """A Variational Autoencoder to learn embeddings from k-mer vectors."""
    def __init__(self, original_dim, latent_dim=32):
        super().__init__()
        self.latent_dim = latent_dim
        self.encoder = keras.Sequential([
            keras.Input(shape=(original_dim,)),
            layers.Dense(256, activation="relu"),
            layers.Dropout(0.2),
            layers.Dense(128, activation="relu"),
            layers.Dense(latent_dim + latent_dim),
        ])
        self.decoder = keras.Sequential([
            keras.Input(shape=(latent_dim,)),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.2),
            layers.Dense(256, activation="relu"),
            layers.Dense(original_dim, activation="sigmoid"),
        ])

    def reparameterize(self, z_mean, z_log_var):
        """The reparameterization trick."""
        batch = tf.shape(z_mean)[0]
        epsilon = tf.random.normal(shape=(batch, self.latent_dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

    def call(self, inputs):
        """The forward pass of the model."""
        z_mean, z_log_var = tf.split(self.encoder(inputs), num_or_size_splits=2, axis=1)
        z = self.reparameterize(z_mean, z_log_var)
        reconstruction = self.decoder(z)
        # Calculate and add the KL divergence loss
        kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
        self.add_loss(kl_loss)
        return reconstruction

    def get_embeddings(self, inputs):
        """Extracts the latent space embeddings (z_mean) from the encoder."""
        z_mean, _ = tf.split(self.encoder(inputs), num_or_size_splits=2, axis=1)
        return z_mean

# ===================================================================
# 2. VAE Training Function âš™ï¸
# ===================================================================
def train_vae_and_get_embeddings(kmer_matrix, epochs=30, batch_size=128):
    """Trains the VAE and returns the learned sequence embeddings."""
    print("\n--- Starting VAE Training ---")
    scaler = StandardScaler()
    kmer_scaled = scaler.fit_transform(kmer_matrix)

    vae = VAE(original_dim=kmer_scaled.shape[1])
    vae.compile(optimizer=keras.optimizers.Adam())

    print(f"Training VAE for {epochs} epochs...")
    vae.fit(kmer_scaled, kmer_scaled, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)

    embeddings = vae.get_embeddings(kmer_scaled)
    print("--- VAE Training and Embedding Extraction Complete ---")
    return embeddings.numpy()

# ===================================================================
# 3. Execution Block ðŸš€
# ===================================================================
# This assumes an object 'pipeline_data' with the attribute 'kmer_vectors'
# was created and populated by running the first cell.
if 'pipeline_data' in locals() and hasattr(pipeline_data, 'kmer_vectors') and pipeline_data.kmer_vectors is not None:
    # Run the VAE training on the k-mer data from Cell 1
    sequence_embeddings = train_vae_and_get_embeddings(pipeline_data.kmer_vectors)

    # Save the embeddings to a file for your partner
    print("\nSaving deliverables...")
    np.save('embeddings.npy', sequence_embeddings)
    print("âœ… Successfully saved 'embeddings.npy'. Your partner can now use this file for clustering.")

else:
    print("âŒ Error: Could not find 'pipeline_data' or its 'kmer_vectors'.")
    print("Please make sure you have successfully run the first cell to generate the data.")

# dbscan_module.py
import numpy as np
import os
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
import math
import random
class eDNABiodiversityPipeline:
  # -------------------------
  # Helper: Load embeddings
  # -------------------------
  def load_embeddings(self,path='embeddings.npy'):
      emb = np.load(path)
      print(f"[INFO] Loaded embeddings shape: {emb.shape}")
      return emb

  # -------------------------
  # Optional: k-distance plot to help pick eps
  # -------------------------
  def plot_k_distance(self,embeddings, k=5, save_path='k_distance.png'):
      """
      Compute distances to the k-th nearest neighbor for each point,
      sort them and plot â€” a common way to visually choose DBSCAN eps.
      """
      scaler = StandardScaler()
      X = scaler.fit_transform(embeddings)
      nbrs = NearestNeighbors(n_neighbors=k).fit(X)
      distances, _ = nbrs.kneighbors(X)
      # distance to k-th neighbor (k-1 index because array starts at 0)
      k_dist = distances[:, k-1]
      k_dist_sorted = np.sort(k_dist)[::-1]  # descending

      plt.figure(figsize=(6,4))
      plt.plot(range(1, len(k_dist_sorted)+1), k_dist_sorted)
      plt.xlabel('Points (sorted by distance)')
      plt.ylabel(f'{k}-th nearest neighbor distance')
      plt.title(f'k-distance plot (k={k}) â€” use elbow for eps')
      plt.tight_layout()
      plt.savefig(save_path, dpi=150)
      plt.close()
      print(f"[INFO] k-distance plot saved to {save_path}. Look for an 'elbow' to pick eps.")

  # -------------------------
  # Core: DBSCAN clustering
  # -------------------------
  def run_dbscan(self,embeddings, eps=0.5, min_samples=5):
      """
      Run DBSCAN on embeddings (automatically scales embeddings).
      Returns: cluster_labels (numpy array)
      """
      scaler = StandardScaler()
      X = scaler.fit_transform(embeddings)

      db = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')
      labels = db.fit_predict(X)
      n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
      n_noise = list(labels).count(-1)
      print(f"[INFO] DBSCAN result -> clusters: {n_clusters}, noise_points: {n_noise}")
      if n_clusters > 1:
            silhouette = silhouette_score(X,labels)
            print(f"  - Silhouette score: {silhouette:.4f}")
      else:
            print("  - Silhouette score: N/A (requires more than one cluster)")

      self.clusters = labels
      return labels

  def analyze_biodiversity(self,cluster_labels):
          """
          Calculate biodiversity metrics from clustering results.

          This function simulates the biological interpretation of clustering results:
          - Each cluster represents a potential taxon (species/genus/family)
          - Noise points represent potentially novel taxa not in existing databases
          - Standard ecological metrics are calculated

          Args:
              cluster_labels (numpy.ndarray): Cluster assignments from DBSCAN

          Returns:
              dict: Comprehensive biodiversity analysis results
          """
          print("Analyzing biodiversity metrics...")

          # Basic cluster statistics
          unique_clusters = set(cluster_labels)

          # Create taxa classification
          taxa_list = []
          cluster_counts = Counter(cluster_labels)

          # Create mock classifications for identified taxa
          mock_classifications = [
              "Eukaryota; Protista; Ciliophora", "Eukaryota; Metazoa; Chordata; Fish",
              "Eukaryota; Metazoa; Echinodermata", "Eukaryota; Metazoa; Mollusca",
              "Eukaryota; Cnidaria; Hydrozoa", "Eukaryota; Metazoa; Annelida",
              "Eukaryota; Porifera; Demospongiae", "Eukaryota; Arthropoda; Crustacea",
              "Eukaryota; Nematoda; Secernentea", "Eukaryota; Fungi; Ascomycota"
          ]

          # Process regular clusters (identified taxa)
          for cluster_id in sorted(unique_clusters):
              count = cluster_counts[cluster_id]

              if cluster_id == -1:
                  # Noise points are handled as novel taxa
                  continue

              # Simulate taxonomic assignment with mock confidence
              classification = random.choice(mock_classifications)

              taxon = {
                  'cluster_id': int(cluster_id),
                  'abundance': int(count),
                  'status': 'Identified',
                  'confidence': round(random.uniform(0.6, 0.95), 3),
                  'taxon_name': f'{classification.split(";")[-1]}_cluster_{cluster_id}',
                  'taxonomic_level': 'Species' if random.random() > 0.3 else 'Genus'
              }
              taxa_list.append(taxon)

          # Process noise points (novel taxa)
          n_noise = cluster_counts.get(-1, 0)
          if n_noise > 0:
              for i in range(max(1, n_noise // 50)): # Group noise points into several novel taxa
                  sequences_in_taxon = max(1, n_noise // max(1, n_noise // 50))

                  taxon = {
                      'cluster_id': f'Novel_{i+1}',
                      'abundance': int(sequences_in_taxon),
                      'status': 'Novel',
                      'confidence': 0.0,
                      'taxon_name': f'Novel_taxon_{i+1}',
                      'taxonomic_level': 'Unknown'
                  }
                  taxa_list.append(taxon)

          # Calculate biodiversity metrics
          abundances = [taxon['abundance'] for taxon in taxa_list]
          total_sequences = sum(abundances)

          # Species richness (number of taxa)
          species_richness = len(taxa_list)

          # Shannon Diversity Index
          shannon_index = 0
          if total_sequences > 0 and species_richness > 1:
              for abundance in abundances:
                  if abundance > 0:
                      p_i = abundance / total_sequences
                      shannon_index -= p_i * np.log(p_i)

          # Simpson's Diversity Index
          simpson_index = 0
          if total_sequences > 0:
              for abundance in abundances:
                  if abundance > 0:
                      p_i = abundance / total_sequences
                      simpson_index += p_i ** 2
          simpson_index = 1 - simpson_index

          # Pielou's Evenness Index
          pielou_evenness = shannon_index / np.log(species_richness) if species_richness > 1 else 0

          # Compile results
          results = {
              'pipeline_info': {
                  'total_sequences_analyzed': int(total_sequences),
                  'clustering_algorithm': 'DBSCAN',
                  'embedding_method': 'Variational Autoencoder',
                  'k_mer_size' : 4
              },
              'taxonomic_summary': {
                  'total_taxa_identified': int(species_richness),
                  'identified_taxa': int(len([t for t in taxa_list if t['status'] == 'Identified'])),
                  'novel_taxa': int(len([t for t in taxa_list if t['status'] == 'Novel'])),
                  'total_clusters': len(set(c for c in cluster_labels if c != -1)),
                  'noise_points': int(n_noise)
              },
              'biodiversity_metrics': {
                  'species_richness': int(species_richness),
                  'shannon_diversity_index': round(shannon_index, 4),
                  'simpson_diversity_index': round(simpson_index, 4),
                  'pielou_evenness_index': round(pielou_evenness, 4)
              },
              'taxa_details': sorted(taxa_list, key=lambda x: x['abundance'], reverse=True)
          }

          print(f"Biodiversity analysis completed:")
          print(f"  - Total taxa: {species_richness}")
          print(f"  - Shannon diversity: {shannon_index:.4f}")
          print(f"  - Novel taxa discovered: {results['taxonomic_summary']['novel_taxa']}")

          return results


  # -------------------------
  # Visualization: 2D scatter (PCA) colored by cluster
  # -------------------------
  def save_cluster_scatter(self,embeddings, labels, save_path='latent_clusters.png'):
      scaler = StandardScaler()
      X = scaler.fit_transform(embeddings)
      pca = PCA(n_components=2, random_state=42)
      X2 = pca.fit_transform(X)

      unique = sorted(set(labels))
      colors = plt.cm.tab10.colors
      plt.figure(figsize=(7,5))
      for i, lab in enumerate(unique):
          mask = labels == lab
          label_name = 'Noise' if lab == -1 else f'Cl {lab}'
          c = colors[i % len(colors)]
          plt.scatter(X2[mask,0], X2[mask,1], s=6, alpha=0.7, label=label_name, color=c)
      plt.title('DBSCAN clusters (PCA 2D)')
      plt.xlabel('PC1')
      plt.ylabel('PC2')
      if len(unique) <= 12:
          plt.legend(markerscale=3, fontsize=8)
      plt.tight_layout()
      plt.savefig(save_path, dpi=150)
      plt.show()
      plt.close()
      print(f"[INFO] Cluster scatter saved to {save_path}")

  # -------------------------
  # Utility: save cluster assignment table
  # -------------------------
  def save_cluster_table(self,labels, out_csv='cluster_assignments.csv'):
      import pandas as pd
      df = pd.DataFrame({'sequence_index': np.arange(len(labels)), 'cluster_label': labels})
      df.to_csv(out_csv, index=False)
      print(f"[INFO] Cluster assignments saved to {out_csv}")

  # -------------------------
  # Example runner (edit values as needed)
  # -------------------------
if __name__ == '__main__':
    pipeline = eDNABiodiversityPipeline()

    emb_path = 'embeddings.npy'   # change path if needed
    embeddings = pipeline.load_embeddings(emb_path)

    pipeline.plot_k_distance(embeddings, k=5, save_path='k_distance.png')

    labels = pipeline.run_dbscan(embeddings, eps=0.5, min_samples=5)

    pipeline.save_cluster_table(labels, out_csv='cluster_assignments.csv')

    summary = pipeline.analyze_biodiversity(labels)
    print(summary)

    pipeline.save_cluster_scatter(embeddings, labels, save_path='latent_clusters.png')